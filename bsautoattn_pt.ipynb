{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/knightz33/.pyenv/versions/3.11.1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 1, 28, 28]), y_train shape: torch.Size([60000])\n",
      "x_test shape: torch.Size([10000, 1, 28, 28]), y_test shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1000)\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "# Define the dimension\n",
    "dim = 8\n",
    "\n",
    "# Define transformations for the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts images to PyTorch tensors ([0, 255] -> [0.0,1.0])\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 60000  # Load all data at once\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# Get the data\n",
    "for images, labels in train_loader:\n",
    "    x_train = images  # [60000, 1, 28, 28]\n",
    "    y_train = labels  # [60000]\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    x_test = images  # [10000, 1, 28, 28]\n",
    "    y_test = labels  # [10000]\n",
    "    break\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "x_test_copy = x_test.clone()  # Save a copy of x_test for later use\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Binarize the images\n",
    "temp_x_train = x_train.clone()\n",
    "temp_x_train[temp_x_train > 0.5] = 1\n",
    "temp_x_train[temp_x_train <= 0.5] = 0\n",
    "\n",
    "temp_x_test = x_test.clone()\n",
    "temp_x_test[temp_x_test > 0.5] = 1\n",
    "temp_x_test[temp_x_test <= 0.5] = 0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.view(-1, 28*28)\n",
    "x_test = x_test.view(-1, 28*28)\n",
    "\n",
    "# Save labels before any modification\n",
    "y_train_save = y_train.clone()\n",
    "y_test_save = y_test.clone()\n",
    "\n",
    "# Add random noise to the training data\n",
    "np.random.seed(1000)\n",
    "a = (np.random.rand(x_train.shape[0], x_train.shape[1]) > 0.3).astype(float)\n",
    "a = torch.from_numpy(a).float()\n",
    "x_train = torch.max(a, x_train)\n",
    "\n",
    "# Convert each input into 8 dimensions\n",
    "def convert_to_8dim(x, dim=8):\n",
    "    x_new = torch.zeros(x.size(0), x.size(1), dim)\n",
    "    a2 = (x * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_new = x_new.view(-1, 28*28*dim)\n",
    "    return x_new\n",
    "\n",
    "x_train = convert_to_8dim(x_train, dim)\n",
    "x_test = convert_to_8dim(x_test, dim)\n",
    "\n",
    "print(f\"x_train shape after conversion: {x_train.shape}\")\n",
    "print(f\"x_test shape after conversion: {x_test.shape}\")\n",
    "\n",
    "# Set targets to inputs (autoencoder) AFTER conversion\n",
    "y_train = x_train.clone()\n",
    "y_test = x_test.clone()\n",
    "\n",
    "print(f\"y_train shape after setting targets: {y_train.shape}\")\n",
    "print(f\"y_test shape after setting targets: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img, title):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.imshow(npimg, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Display original images (before any transformations)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+1)\n",
    "    imshow(x_test_copy[idx].squeeze(), 'Original')\n",
    "\n",
    "# Display transformed images (after 8-dim conversion)\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+9)\n",
    "    imshow(x_test[idx][0].squeeze(), 'Transformed (Dim 1)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        #self.activation = nn.LeakyReLU()\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.activation(self.conv(x))\n",
    "\n",
    "\n",
    "class AutoEncoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_channels=8):\n",
    "        super(AutoEncoderWithAttention, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(64, 128, kernel_size=7),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        # Residual block\n",
    "        self.res_block = ResidualBlock(128)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=7),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Mish(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Mish(),\n",
    "            nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = self.res_block(x)\n",
    "        \n",
    "        if return_intermediate:\n",
    "            return x\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    # def clamp_weights(self):\n",
    "    #     # Enforce non-negative weights\n",
    "    #     for m in self.modules():\n",
    "    #         if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "    #             m.weight.data.clamp_(0)\n",
    "    \n",
    "    def reconstruct_image(self, x_out):\n",
    "        x_out2 = x_out.view(-1, 28, 28, self.input_channels)\n",
    "        x_out3 = sum([x_out2[:, :, :, i] * (2 ** i) for i in range(self.input_channels)]) / 256\n",
    "        x_out3 = x_out3.view(-1, 28*28)\n",
    "        return x_out3\n",
    "\n",
    "# Instantiate the model with 8 input channels\n",
    "model = AutoEncoderWithAttention(input_channels=8).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss: 0.2283834235308071\n",
      "Epoch [2/150], Loss: 0.03509272433041284\n",
      "Epoch [3/150], Loss: 0.009738549287818993\n",
      "Epoch [4/150], Loss: 0.0037833578675054015\n",
      "Epoch [5/150], Loss: 0.0017707622527571706\n",
      "Epoch [6/150], Loss: 0.0009600954924341447\n",
      "Epoch [7/150], Loss: 0.0005963727989486263\n",
      "Epoch [8/150], Loss: 0.00042293055806415697\n",
      "Epoch [9/150], Loss: 0.00033768542016332503\n",
      "Epoch [10/150], Loss: 0.0002949236296141559\n",
      "Epoch [11/150], Loss: 0.0002732271991529463\n",
      "Epoch [12/150], Loss: 0.00026210700589823923\n",
      "Epoch [13/150], Loss: 0.0002563557880572868\n",
      "Epoch [14/150], Loss: 0.00025337051418622045\n",
      "Epoch [15/150], Loss: 0.000251822674408686\n",
      "Epoch [16/150], Loss: 0.000251010154826569\n",
      "Epoch [17/150], Loss: 0.00025058544228462174\n",
      "Epoch [18/150], Loss: 0.000250362697336944\n",
      "Epoch [19/150], Loss: 0.00025024501236354504\n",
      "Epoch [20/150], Loss: 0.0002501829039162355\n",
      "Epoch [21/150], Loss: 0.000250149531311763\n",
      "Epoch [22/150], Loss: 0.00025013155095621187\n",
      "Epoch [23/150], Loss: 0.00025012164057443444\n",
      "Epoch [24/150], Loss: 0.0002501160111508701\n",
      "Epoch [25/150], Loss: 0.00025011279651153016\n",
      "Epoch [26/150], Loss: 0.0002501108737002748\n",
      "Epoch [27/150], Loss: 0.00025010967979748484\n",
      "Epoch [28/150], Loss: 0.00025010890251299617\n",
      "Epoch [29/150], Loss: 0.00025010837596407023\n",
      "Epoch [30/150], Loss: 0.0002501080161558396\n",
      "Epoch [31/150], Loss: 0.00025010776201573513\n",
      "Epoch [32/150], Loss: 0.00025010756829942695\n",
      "Epoch [33/150], Loss: 0.000250107418026649\n",
      "Epoch [34/150], Loss: 0.0002501072957723712\n",
      "Epoch [35/150], Loss: 0.00025010720537466113\n",
      "Epoch [36/150], Loss: 0.00025010711903329744\n",
      "Epoch [37/150], Loss: 0.00025010704702557024\n",
      "Epoch [38/150], Loss: 0.00025010699848280636\n",
      "Epoch [39/150], Loss: 0.000250106947335856\n",
      "Epoch [40/150], Loss: 0.00025010689847325315\n",
      "Epoch [41/150], Loss: 0.00025010685333048363\n",
      "Epoch [42/150], Loss: 0.0002501068198913951\n",
      "Epoch [43/150], Loss: 0.0002501067927338833\n",
      "Epoch [44/150], Loss: 0.000250106769035483\n",
      "Epoch [45/150], Loss: 0.0002501096999731089\n",
      "Epoch [46/150], Loss: 0.0002501069539145343\n",
      "Epoch [47/150], Loss: 0.0002501069430642625\n",
      "Epoch [48/150], Loss: 0.0002501067650700861\n",
      "Epoch [49/150], Loss: 0.00025010671426571205\n",
      "Epoch [50/150], Loss: 0.0002501066858045912\n",
      "Epoch [51/150], Loss: 0.000250106659859739\n",
      "Epoch [52/150], Loss: 0.0002501066421579405\n",
      "Epoch [53/150], Loss: 0.0002501066237922108\n",
      "Epoch [54/150], Loss: 0.0002501066075289297\n",
      "Epoch [55/150], Loss: 0.0002501065937394742\n",
      "Epoch [56/150], Loss: 0.00025010658210097364\n",
      "Epoch [57/150], Loss: 0.0002501065694286808\n",
      "Epoch [58/150], Loss: 0.0002501065610855828\n",
      "Epoch [59/150], Loss: 0.00025010655315478895\n",
      "Epoch [60/150], Loss: 0.00025010654406742105\n",
      "Epoch [61/150], Loss: 0.00025010653635339013\n",
      "Epoch [62/150], Loss: 0.0002501065305144342\n",
      "Epoch [63/150], Loss: 0.0002501065234901034\n",
      "Epoch [64/150], Loss: 0.0002501065173813307\n",
      "Epoch [65/150], Loss: 0.00025010651149083666\n",
      "Epoch [66/150], Loss: 0.0002501065058155897\n",
      "Epoch [67/150], Loss: 0.00025010650075879917\n",
      "Epoch [68/150], Loss: 0.0002501064947515867\n",
      "Epoch [69/150], Loss: 0.00025010649023291383\n",
      "Epoch [70/150], Loss: 0.00025010648493965467\n",
      "Epoch [71/150], Loss: 0.00025010648060136496\n",
      "Epoch [72/150], Loss: 0.00025010647312228684\n",
      "Epoch [73/150], Loss: 0.0002501064681018761\n",
      "Epoch [74/150], Loss: 0.00025010646074406393\n",
      "Epoch [75/150], Loss: 0.00025010645536591863\n",
      "Epoch [76/150], Loss: 0.00025010645173703476\n",
      "Epoch [77/150], Loss: 0.00025010644705920036\n",
      "Epoch [78/150], Loss: 0.0002501064449340144\n",
      "Epoch [79/150], Loss: 0.00025010643901017223\n",
      "Epoch [80/150], Loss: 0.0002501064355813772\n",
      "Epoch [81/150], Loss: 0.0002501064318494173\n",
      "Epoch [82/150], Loss: 0.000250106429269484\n",
      "Epoch [83/150], Loss: 0.00025010642515553626\n",
      "Epoch [84/150], Loss: 0.00025010642259985614\n",
      "Epoch [85/150], Loss: 0.00025010641910284904\n",
      "Epoch [86/150], Loss: 0.00025010641378988413\n",
      "Epoch [87/150], Loss: 0.00025010641356857375\n",
      "Epoch [88/150], Loss: 0.000250106409325781\n",
      "Epoch [89/150], Loss: 0.0002501064049768805\n",
      "Epoch [90/150], Loss: 0.0002501064023302509\n",
      "Epoch [91/150], Loss: 0.0002501063980253093\n",
      "Epoch [92/150], Loss: 0.0002501063945783244\n",
      "Epoch [93/150], Loss: 0.0002501063919983911\n",
      "Epoch [94/150], Loss: 0.0002501063889121724\n",
      "Epoch [95/150], Loss: 0.0002501063851893074\n",
      "Epoch [96/150], Loss: 0.00025010638311869117\n",
      "Epoch [97/150], Loss: 0.0002501063814512842\n",
      "Epoch [98/150], Loss: 0.00025010637974749744\n",
      "Epoch [99/150], Loss: 0.000250106377989141\n",
      "Epoch [100/150], Loss: 0.00025010637675677573\n",
      "Epoch [101/150], Loss: 0.00025010637581848033\n",
      "Epoch [102/150], Loss: 0.000250106372902034\n",
      "Epoch [103/150], Loss: 0.0002501063710618231\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# model.clamp_weights()\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.1/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # model.clamp_weights()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "# Extract intermediate outputs for visualization\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_mid = model(x_test.to(device), return_intermediate=True)  \n",
    "test_mid_np = test_mid.cpu().numpy()\n",
    "\n",
    "print(f\"Shape of intermediate outputs: {test_mid_np.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the model, define the optimizer and loss function\n",
    "model = Autoencoder()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Prepare data loaders for training\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clamp_weights()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "# Extract intermediate outputs for visualization\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_mid = model(x_test, return_intermediate=True)\n",
    "test_mid_np = test_mid.cpu().numpy()\n",
    "\n",
    "# SVD whitening\n",
    "def svd_whiten(X):\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    X_white = np.dot(U, Vt)\n",
    "    return X_white\n",
    "\n",
    "test_mid_np = svd_whiten(test_mid_np)\n",
    "print(f\"Whitened shape: {test_mid_np.shape}\")\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "test_mid2 = pca.fit_transform(test_mid_np)\n",
    "\n",
    "\n",
    "\n",
    "# Plot PCA results\n",
    "labels = y_test_save.numpy()\n",
    "for i in range(10):\n",
    "    plt.scatter(test_mid2[labels == i, 0], test_mid2[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# t-SNE visualization\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, verbose=1).fit_transform(test_mid_np)\n",
    "for i in range(10):\n",
    "    plt.scatter(X_embedded[labels == i, 0], X_embedded[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Fit a robust covariance estimator\n",
    "robust_cov = MinCovDet().fit(test_mid_np)\n",
    "cov = robust_cov.covariance_\n",
    "mea = np.mean(test_mid_np, axis=0)\n",
    "\n",
    "# Function to binarize images\n",
    "def binarized(x_test):\n",
    "    x_test_new = torch.zeros(x_test.size(0), x_test.size(1), dim)\n",
    "    a2 = (x_test * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_test_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_test_new = x_test_new.view(-1, 28*28*dim)\n",
    "    return x_test_new\n",
    "\n",
    "# Generate images by interpolating between two random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 2\n",
    "b = (np.floor(np.random.rand(n) * len(x_test))).astype(int)\n",
    "x_test_tempo = x_test[b, :].clone()\n",
    "with torch.no_grad():\n",
    "    test_mid = model(x_test_tempo, return_intermediate=True)\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    b2 = np.array([i / 99, 1 - i / 99])\n",
    "    temp = (test_mid * torch.from_numpy(b2).float().view(n, 1)).sum(dim=0).unsqueeze(0)\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Interpolation between two images')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Generate images from random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 1\n",
    "with torch.no_grad():\n",
    "    test_mid2 = np.random.multivariate_normal(mea + (2 * np.random.rand(100) - 1), cov, size=1)\n",
    "    test_mid = torch.from_numpy(test_mid2).float()\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    temp = test_mid\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Generated images from random latent vectors')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img * 0.5 + 0.5  # Unnormalize to [0,1]\n",
    "    npimg = img.cpu().numpy()\n",
    "    # Select the first channel (if grayscale) or handle the display for multi-channel data\n",
    "    plt.imshow(npimg[0], cmap='gray')  # Displaying the first channel\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(dataiter)  \n",
    "images = images.to(device)\n",
    "\n",
    "# Get reconstructed images\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(device))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+1)\n",
    "    imshow(images[idx], 'Original')  # Display only the first channel\n",
    "\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+9)\n",
    "    imshow(outputs[idx], 'Reconstructed')  # Display only the first channel\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(U, Vt)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n\u001b[0;32m----> 7\u001b[0m test_mid_np \u001b[38;5;241m=\u001b[39m \u001b[43msvd_whiten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_mid_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhitened shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mid_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# PCA for visualization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36msvd_whiten\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd_whiten\u001b[39m(X):\n\u001b[1;32m      3\u001b[0m     U, s, Vt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVD whitening\n",
    "def svd_whiten(X):\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    X_white = np.dot(U, Vt)\n",
    "    return X_white\n",
    "\n",
    "test_mid_np = svd_whiten(test_mid_np)\n",
    "print(f\"Whitened shape: {test_mid_np.shape}\")\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "test_mid2 = pca.fit_transform(test_mid_np)\n",
    "\n",
    "\n",
    "\n",
    "# Plot PCA results\n",
    "labels = y_test_save.numpy()\n",
    "for i in range(10):\n",
    "    plt.scatter(test_mid2[labels == i, 0], test_mid2[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# t-SNE visualization\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, verbose=1).fit_transform(test_mid_np)\n",
    "for i in range(10):\n",
    "    plt.scatter(X_embedded[labels == i, 0], X_embedded[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Fit a robust covariance estimator\n",
    "robust_cov = MinCovDet().fit(test_mid_np)\n",
    "cov = robust_cov.covariance_\n",
    "mea = np.mean(test_mid_np, axis=0)\n",
    "\n",
    "# Function to binarize images\n",
    "def binarized(x_test):\n",
    "    x_test_new = torch.zeros(x_test.size(0), x_test.size(1), dim)\n",
    "    a2 = (x_test * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_test_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_test_new = x_test_new.view(-1, 28*28*dim)\n",
    "    return x_test_new\n",
    "\n",
    "# Generate images by interpolating between two random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 2\n",
    "b = (np.floor(np.random.rand(n) * len(x_test))).astype(int)\n",
    "x_test_tempo = x_test[b, :].clone()\n",
    "with torch.no_grad():\n",
    "    test_mid = model(x_test_tempo, return_intermediate=True)\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    b2 = np.array([i / 99, 1 - i / 99])\n",
    "    temp = (test_mid * torch.from_numpy(b2).float().view(n, 1)).sum(dim=0).unsqueeze(0)\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Interpolation between two images')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate images from random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 1\n",
    "with torch.no_grad():\n",
    "    test_mid2 = np.random.multivariate_normal(mea + (2 * np.random.rand(100) - 1), cov, size=1)\n",
    "    test_mid = torch.from_numpy(test_mid2).float()\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    temp = test_mid\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Generated images from random latent vectors')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
