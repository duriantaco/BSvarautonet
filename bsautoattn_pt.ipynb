{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([60000, 1, 28, 28]), y_train shape: torch.Size([60000])\n",
      "x_test shape: torch.Size([10000, 1, 28, 28]), y_test shape: torch.Size([10000])\n",
      "x_train shape after conversion: torch.Size([60000, 6272])\n",
      "x_test shape after conversion: torch.Size([10000, 6272])\n",
      "y_train shape after setting targets: torch.Size([60000, 6272])\n",
      "y_test shape after setting targets: torch.Size([10000, 6272])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EmpiricalCovariance, MinCovDet\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1000)\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "# Define the dimension\n",
    "dim = 8\n",
    "\n",
    "# Define transformations for the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts images to PyTorch tensors ([0, 255] -> [0.0,1.0])\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "\n",
    "# Get the data\n",
    "for images, labels in train_loader:\n",
    "    x_train = images  # [60000, 1, 28, 28]\n",
    "    y_train = labels  # [60000]\n",
    "    break\n",
    "\n",
    "for images, labels in test_loader:\n",
    "    x_test = images  # [10000, 1, 28, 28]\n",
    "    y_test = labels  # [10000]\n",
    "    break\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "x_test_copy = x_test.clone()  # Save a copy of x_test for later use\n",
    "\n",
    "# Normalize the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Binarize the images\n",
    "temp_x_train = x_train.clone()\n",
    "temp_x_train[temp_x_train > 0.5] = 1\n",
    "temp_x_train[temp_x_train <= 0.5] = 0\n",
    "\n",
    "temp_x_test = x_test.clone()\n",
    "temp_x_test[temp_x_test > 0.5] = 1\n",
    "temp_x_test[temp_x_test <= 0.5] = 0\n",
    "\n",
    "# Flatten the images\n",
    "x_train = x_train.view(-1, 28*28)\n",
    "x_test = x_test.view(-1, 28*28)\n",
    "\n",
    "# Save labels before any modification\n",
    "y_train_save = y_train.clone()\n",
    "y_test_save = y_test.clone()\n",
    "\n",
    "# Add random noise to the training data\n",
    "np.random.seed(1000)\n",
    "a = (np.random.rand(x_train.shape[0], x_train.shape[1]) > 0.3).astype(float)\n",
    "a = torch.from_numpy(a).float()\n",
    "x_train = torch.max(a, x_train)\n",
    "\n",
    "# Convert each input into 8 dimensions\n",
    "def convert_to_8dim(x, dim=8):\n",
    "    x_new = torch.zeros(x.size(0), x.size(1), dim)\n",
    "    a2 = (x * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_new = x_new.view(-1, 28*28*dim)\n",
    "    return x_new\n",
    "\n",
    "x_train = convert_to_8dim(x_train, dim)\n",
    "x_test = convert_to_8dim(x_test, dim)\n",
    "\n",
    "print(f\"x_train shape after conversion: {x_train.shape}\")\n",
    "print(f\"x_test shape after conversion: {x_test.shape}\")\n",
    "\n",
    "# Set targets to inputs (autoencoder) AFTER conversion\n",
    "y_train = x_train.clone()\n",
    "y_test = x_test.clone()\n",
    "\n",
    "print(f\"y_train shape after setting targets: {y_train.shape}\")\n",
    "print(f\"y_test shape after setting targets: {y_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the autoencoder model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Residual Block definition\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.activation = nn.Mish()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.activation(self.conv(x))\n",
    "\n",
    "# AutoEncoder Model definition\n",
    "class AutoEncoderWithAttention(nn.Module):\n",
    "    def __init__(self, input_channels=8):\n",
    "        super(AutoEncoderWithAttention, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Mish(),\n",
    "            nn.Conv2d(64, 128, kernel_size=7),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        # Residual Block\n",
    "        self.res_block = ResidualBlock(128)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=7),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Mish(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Mish(),\n",
    "            nn.ConvTranspose2d(32, input_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        x = self.encoder(x)\n",
    "        x = self.res_block(x)\n",
    "        if return_intermediate:\n",
    "            return x\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def reconstruct_image(self, x_out):\n",
    "        x_out2 = x_out.view(-1, 28, 28, self.input_channels)\n",
    "        x_out3 = sum([x_out2[:, :, :, i] * (2 ** i) for i in range(self.input_channels)]) / 256\n",
    "        x_out3 = x_out3.view(-1, 28 * 28)\n",
    "        return x_out3\n",
    "\n",
    "# Instantiate the model\n",
    "device = torch.device('cpu')\n",
    "model = AutoEncoderWithAttention(input_channels=8).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0002514817981379262\n",
      "Epoch [2/10], Loss: 0.00025083374289655086\n",
      "Epoch [3/10], Loss: 0.00025049286197827316\n",
      "Epoch [4/10], Loss: 0.00025031391400261784\n",
      "Epoch [5/10], Loss: 0.0002502195637892631\n",
      "Epoch [6/10], Loss: 0.00025016920350935836\n",
      "Epoch [7/10], Loss: 0.0002501422318755431\n",
      "Epoch [8/10], Loss: 0.00025012756554133376\n",
      "Epoch [9/10], Loss: 0.0002501194079710937\n",
      "Epoch [10/10], Loss: 0.0002501147727404411\n",
      "Shape of intermediate outputs: (10000, 128, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "test_dataset = TensorDataset(x_test, y_test)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "# Training loop (with reshaping)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Reshape inputs and targets to 4D for the convolutional layers\n",
    "        inputs = inputs.view(-1, dim, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "        targets = targets.view(-1, dim, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader)}\")\n",
    "\n",
    "# Extract intermediate outputs for visualization (with reshaping)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = x_test.view(-1, dim, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "    test_mid = model(x_test.to(device), return_intermediate=True)\n",
    "\n",
    "test_mid_np = test_mid.cpu().numpy()\n",
    "print(f\"Shape of intermediate outputs: {test_mid_np.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of intermediate outputs: (10000, 128, 1, 1)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(U, Vt)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n\u001b[0;32m---> 32\u001b[0m test_mid_np \u001b[38;5;241m=\u001b[39m \u001b[43msvd_whiten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_mid_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhitened shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mid_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# PCA for visualization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m, in \u001b[0;36msvd_whiten\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd_whiten\u001b[39m(X):\n\u001b[1;32m     28\u001b[0m     U, s, Vt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 29\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "# Function to binarize images\n",
    "def binarized(x_test, dim=8):\n",
    "    x_test_new = torch.zeros(x_test.size(0), x_test.size(1), dim)\n",
    "    a2 = (x_test * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_test_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_test_new = x_test_new.view(-1, 28*28*dim)\n",
    "    return x_test_new\n",
    "\n",
    "# Extract intermediate outputs for visualization\n",
    "def extract_intermediate_outputs(model, x_test, dim=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_test = x_test.view(-1, dim, 28, 28)  # Reshape to 4D [batch_size, channels, height, width]\n",
    "        test_mid = model(x_test.to(device), return_intermediate=True)\n",
    "    return test_mid.cpu().numpy()\n",
    "\n",
    "# SVD whitening function\n",
    "def svd_whiten(X):\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    X_white = np.dot(U, Vt)\n",
    "    return X_white\n",
    "\n",
    "# PCA for visualization\n",
    "def plot_pca(test_mid_np, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    test_mid2 = pca.fit_transform(test_mid_np)\n",
    "    for i in range(10):\n",
    "        plt.scatter(test_mid2[labels == i, 0], test_mid2[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# t-SNE visualization\n",
    "def plot_tsne(test_mid_np, labels):\n",
    "    X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, verbose=1).fit_transform(test_mid_np)\n",
    "    for i in range(10):\n",
    "        plt.scatter(X_embedded[labels == i, 0], X_embedded[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Fit a robust covariance estimator\n",
    "def fit_robust_covariance(test_mid_np):\n",
    "    robust_cov = MinCovDet().fit(test_mid_np)\n",
    "    cov = robust_cov.covariance_\n",
    "    mea = np.mean(test_mid_np, axis=0)\n",
    "    return cov, mea\n",
    "\n",
    "# Generate images by interpolating between two random latent vectors\n",
    "def interpolate_between_latents(model, x_test, n=2, steps=100):\n",
    "    np.random.seed(int(time.time()))\n",
    "    indices = np.random.choice(len(x_test), n, replace=False)\n",
    "    x_test_tempo = x_test[indices].clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_mid = model(x_test_tempo.view(-1, 8, 28, 28).to(device), return_intermediate=True)\n",
    "\n",
    "    test_out_big = np.zeros((28, 28 * steps))\n",
    "\n",
    "    for i in range(steps):\n",
    "        alpha = i / (steps - 1)\n",
    "        interpolated_mid = (1 - alpha) * test_mid[0] + alpha * test_mid[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_out = model.decoder(interpolated_mid.unsqueeze(0))\n",
    "\n",
    "        test_out_img = model.reconstruct_image(test_out)\n",
    "        test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "        test_out_big[:, i * 28:(i + 1) * 28] = test_out_img\n",
    "\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    plt.imshow(test_out_big, cmap='gray')\n",
    "    plt.title('Interpolation between two images')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate images from random latent vectors\n",
    "def generate_random_images(model, cov, mea, num_images=100):\n",
    "    np.random.seed(int(time.time()))\n",
    "    test_out_big = np.zeros((28, 28 * num_images))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_images):\n",
    "            test_mid2 = np.random.multivariate_normal(mea, cov, size=1)\n",
    "            test_mid = torch.from_numpy(test_mid2).float().to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                test_out = model.decoder(test_mid.view(1, 128, 1, 1))\n",
    "\n",
    "            test_out_img = model.reconstruct_image(test_out)\n",
    "            test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "            test_out_big[:, i * 28:(i + 1) * 28] = test_out_img\n",
    "\n",
    "    plt.figure(figsize=(15, 2))\n",
    "    plt.imshow(test_out_big, cmap='gray')\n",
    "    plt.title('Generated images from random latent vectors')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img, title):\n",
    "    img = img * 0.5 + 0.5  # Unnormalize to [0,1]\n",
    "    npimg = img.cpu().numpy()\n",
    "    # Select the first channel (if grayscale) or handle the display for multi-channel data\n",
    "    plt.imshow(npimg[0], cmap='gray')  # Displaying the first channel\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, _ = next(dataiter)  \n",
    "images = images.to(device)\n",
    "\n",
    "# Get reconstructed images\n",
    "with torch.no_grad():\n",
    "    outputs = model(images.to(device))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+1)\n",
    "    imshow(images[idx], 'Original')  # Display only the first channel\n",
    "\n",
    "for idx in range(8):\n",
    "    plt.subplot(2, 8, idx+9)\n",
    "    imshow(outputs[idx], 'Reconstructed')  # Display only the first channel\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(U, Vt)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n\u001b[0;32m----> 7\u001b[0m test_mid_np \u001b[38;5;241m=\u001b[39m \u001b[43msvd_whiten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_mid_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhitened shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mid_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# PCA for visualization\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m, in \u001b[0;36msvd_whiten\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd_whiten\u001b[39m(X):\n\u001b[1;32m      3\u001b[0m     U, s, Vt \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m     X_white \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_white\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 5.96 TiB for an array with shape (10000, 128, 1, 10000, 128, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "\n",
    "# SVD whitening\n",
    "def svd_whiten(X):\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    X_white = np.dot(U, Vt)\n",
    "    return X_white\n",
    "\n",
    "test_mid_np = svd_whiten(test_mid_np)\n",
    "print(f\"Whitened shape: {test_mid_np.shape}\")\n",
    "\n",
    "# PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "test_mid2 = pca.fit_transform(test_mid_np)\n",
    "\n",
    "\n",
    "\n",
    "# Plot PCA results\n",
    "labels = y_test_save.numpy()\n",
    "for i in range(10):\n",
    "    plt.scatter(test_mid2[labels == i, 0], test_mid2[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# t-SNE visualization\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=3, verbose=1).fit_transform(test_mid_np)\n",
    "for i in range(10):\n",
    "    plt.scatter(X_embedded[labels == i, 0], X_embedded[labels == i, 1], s=1, label=f\"Digit {i}\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Fit a robust covariance estimator\n",
    "robust_cov = MinCovDet().fit(test_mid_np)\n",
    "cov = robust_cov.covariance_\n",
    "mea = np.mean(test_mid_np, axis=0)\n",
    "\n",
    "# Function to binarize images\n",
    "def binarized(x_test):\n",
    "    x_test_new = torch.zeros(x_test.size(0), x_test.size(1), dim)\n",
    "    a2 = (x_test * ((2**dim)*1 - 1e-10)).int()\n",
    "    for i in range(dim):\n",
    "        x_test_new[:, :, i] = a2 % 2\n",
    "        a2 = a2 // 2\n",
    "    x_test_new = x_test_new.view(-1, 28*28*dim)\n",
    "    return x_test_new\n",
    "\n",
    "# Generate images by interpolating between two random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 2\n",
    "b = (np.floor(np.random.rand(n) * len(x_test))).astype(int)\n",
    "x_test_tempo = x_test[b, :].clone()\n",
    "with torch.no_grad():\n",
    "    test_mid = model(x_test_tempo, return_intermediate=True)\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    b2 = np.array([i / 99, 1 - i / 99])\n",
    "    temp = (test_mid * torch.from_numpy(b2).float().view(n, 1)).sum(dim=0).unsqueeze(0)\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Interpolation between two images')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate images from random latent vectors\n",
    "np.random.seed(int(time.time()))\n",
    "n = 1\n",
    "with torch.no_grad():\n",
    "    test_mid2 = np.random.multivariate_normal(mea + (2 * np.random.rand(100) - 1), cov, size=1)\n",
    "    test_mid = torch.from_numpy(test_mid2).float()\n",
    "\n",
    "test_out_big = np.zeros((28*1, 28*100))\n",
    "for i in range(100):\n",
    "    temp = test_mid\n",
    "    temp_neg = -temp\n",
    "    temp_cat = torch.cat((temp, temp_neg), dim=1)\n",
    "    with torch.no_grad():\n",
    "        test_out = torch.sigmoid(model.fc_out(temp_cat))\n",
    "    \n",
    "    # Include binarization steps\n",
    "    for repeat in range(5):\n",
    "        a = (np.random.rand(test_out.shape[0], test_out.shape[1]) > 0.4).astype(float)\n",
    "        a = torch.from_numpy(a).float()\n",
    "        test_out = torch.max(a, test_out)\n",
    "        test_out = binarized(test_out)\n",
    "        with torch.no_grad():\n",
    "            test_out = model(test_out)\n",
    "    \n",
    "    test_out_img = model.reconstruct_image(test_out)\n",
    "    test_out_img = test_out_img.cpu().numpy().reshape(28, 28)\n",
    "    test_out_big[0*28:(0+1)*28, i*28:(i+1)*28] = test_out_img\n",
    "\n",
    "plt.figure(figsize=(15, 2))\n",
    "plt.imshow(test_out_big, cmap='gray')\n",
    "plt.title('Generated images from random latent vectors')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
